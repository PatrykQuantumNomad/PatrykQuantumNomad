---
phase: 28-data-foundation
plan: 02
type: execute
wave: 2
depends_on:
  - 28-01
files_modified:
  - src/data/db-compass/models.json
autonomous: true
requirements:
  - DATA-01
  - DATA-02
  - DATA-03
  - DATA-04
  - DATA-07

must_haves:
  truths:
    - "All 12 database model entries exist in models.json and pass Zod validation at build time"
    - "Every model has scores across all 8 dimensions with per-dimension justification text"
    - "Multi-model databases (Redis, PostgreSQL, Cosmos DB, MongoDB, DynamoDB) have crossCategory fields linking to secondary model types"
    - "Each model lists 3-6 top databases with name, description, license, and valid external URL"
    - "Each model has a CAP theorem classification (CP/AP/CA/Tunable) with explanatory notes"
  artifacts:
    - path: "src/data/db-compass/models.json"
      provides: "Complete data for 12 database model categories"
      contains: "key-value"
      min_lines: 500
  key_links:
    - from: "src/data/db-compass/models.json"
      to: "src/lib/db-compass/schema.ts"
      via: "Zod validation at build time through content collection"
      pattern: "dbModelSchema"
    - from: "src/data/db-compass/models.json"
      to: "src/content.config.ts"
      via: "file() loader loading and validating the JSON array"
      pattern: "file.*db-compass/models.json"
---

<objective>
Author all 12 database model entries in models.json with complete scores, justifications, CAP profiles, top databases, and cross-category metadata.

Purpose: This is the content backbone of Database Compass. Without fully populated, well-calibrated data, no visualization or page can deliver value. Each entry must withstand scrutiny from database professionals -- scores need justification, multi-model databases need cross-category links, and top databases need accurate descriptions and valid URLs.

Output: A single `models.json` file containing 12 validated entries (~800-1000 lines) that `astro build` successfully loads through the Zod schema from Plan 01.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/28-data-foundation/28-RESEARCH.md
@.planning/phases/28-data-foundation/28-01-SUMMARY.md
@src/lib/db-compass/schema.ts
@src/lib/db-compass/dimensions.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Author first 6 database model entries</name>
  <files>src/data/db-compass/models.json</files>
  <action>
Replace the empty `[]` placeholder in `src/data/db-compass/models.json` with the first 6 of 12 database model entries. Each entry must conform to the `dbModelSchema` from Plan 01.

**Scoring methodology -- CRITICAL:**
Score all 6 models on ONE dimension at a time (all scalability scores first, then all performance scores, etc.) to ensure consistent calibration across models. Do NOT score one model fully before moving to the next. This prevents calibration drift.

**Score rubric:**
- 1-2: Fundamentally weak in this dimension
- 3-4: Below average, notable limitations
- 5-6: Average, competent but not a strength
- 7-8: Strong, a clear advantage
- 9-10: Exceptional, among the best possible

Use the FULL 1-10 range. Avoid clustering everything at 5-7.

**The first 6 models (ordered by complexityPosition):**

1. **key-value** (complexityPosition: 0.08)
   - Name: "Key-Value Store"
   - Use the complete example entry from 28-RESEARCH.md as the template
   - crossCategory: `[]` (pure key-value stores)
   - CAP: Tunable
   - Top databases: Redis, Amazon DynamoDB, Memcached, etcd

2. **document** (complexityPosition: 0.22)
   - Name: "Document Database"
   - crossCategory: `[]`
   - CAP: AP or Tunable (MongoDB supports configurable consistency)
   - Top databases: MongoDB, Couchbase, Amazon DocumentDB, Firestore (3-6 entries)
   - Key traits: schema-flexible, JSON/BSON documents, nested data, good query flexibility compared to KV

3. **in-memory** (complexityPosition: 0.30)
   - Name: "In-Memory Database"
   - crossCategory: `["key-value"]` (many in-memory DBs are also KV stores)
   - CAP: Tunable
   - Top databases: Redis (in-memory mode), Memcached, VoltDB, Apache Ignite, SAP HANA
   - Key traits: microsecond latency, volatile by default unless persisted, real-time analytics

4. **time-series** (complexityPosition: 0.35)
   - Name: "Time-Series Database"
   - crossCategory: `[]`
   - CAP: Tunable
   - Top databases: InfluxDB, TimescaleDB, QuestDB, Amazon Timestream
   - Key traits: append-optimized, time-windowed queries, downsampling, IoT/monitoring focus

5. **relational** (complexityPosition: 0.42)
   - Name: "Relational (SQL) Database"
   - crossCategory: `["document"]` (PostgreSQL supports JSONB document storage)
   - CAP: CA (single-node default, but distributed SQL variants exist)
   - Top databases: PostgreSQL, MySQL, Microsoft SQL Server, Oracle Database, SQLite
   - Key traits: ACID, joins, mature ecosystem, SQL standard, highest query flexibility

6. **search** (complexityPosition: 0.45)
   - Name: "Search Engine"
   - crossCategory: `["document"]` (search engines store JSON documents internally)
   - CAP: AP
   - Top databases: Elasticsearch, Apache Solr, Meilisearch, Typesense
   - Key traits: inverted index, full-text search, relevance scoring, log analytics

**For each entry, include:**
- `id`, `name`, `slug` (slug matches id)
- `icon` -- descriptive icon name (e.g., "key", "file-text", "clock", "database", "search")
- `complexityPosition` -- exact value from research table
- `summary` -- 1-2 sentence model description (what it is, what it optimizes for)
- `characterSketch` -- 2-3 sentence personality sketch (like the Beauty Index character sketches)
- `scores` -- all 8 dimensions, calibrated using the rubric above
- `justifications` -- 1-2 sentences per dimension explaining the score. Must reference specific technical characteristics, not vague claims. Bad: "It's good at scaling." Good: "Horizontal scaling is trivial -- consistent hashing distributes keys with near-linear throughput gains."
- `capTheorem` -- classification + 1-2 sentence notes explaining nuance
- `crossCategory` -- array of related model IDs (use the exact ID strings)
- `strengths` -- 3-5 specific advantages
- `weaknesses` -- 2-5 specific limitations
- `bestFor` -- 2-6 recommended use cases
- `avoidWhen` -- 1-4 anti-patterns
- `topDatabases` -- 3-6 entries with name, description (1-2 sentences), license (exact license name), url (DB-Engines URL format: `https://db-engines.com/en/system/{Name}` with spaces as `+`, or official website URL if not on DB-Engines)
- `useCases` -- 2-6 short labels

**DB-Engines URL format:** `https://db-engines.com/en/system/{Name}` with spaces replaced by `+`. Examples: `Redis`, `Amazon+DynamoDB`, `PostgreSQL`, `Microsoft+SQL+Server`. For databases not on DB-Engines (e.g., Firestore, Typesense), use the official website URL.
  </action>
  <verify>
Run `npx astro build 2>&1 | tail -30` to verify Zod validation passes for all 6 entries. Check for any schema validation errors. The build should succeed or show only warnings unrelated to dbModels.
  </verify>
  <done>
6 database model entries exist in models.json, all passing Zod validation. Each has 8 scored dimensions with justifications, CAP profiles, crossCategory metadata, and 3-6 top databases with valid URLs.
  </done>
</task>

<task type="auto">
  <name>Task 2: Author remaining 6 database model entries</name>
  <files>src/data/db-compass/models.json</files>
  <action>
Add the remaining 6 database model entries to `src/data/db-compass/models.json`, bringing the total to 12.

**Continue the same scoring methodology:** Score all 6 new models on one dimension at a time. ALSO re-verify calibration against the first 6 models -- scan each dimension column across all 12 models to ensure consistent scoring.

**The remaining 6 models (ordered by complexityPosition):**

7. **columnar** (complexityPosition: 0.58)
   - Name: "Wide-Column Store"
   - crossCategory: `[]`
   - CAP: AP
   - Top databases: Apache Cassandra, Apache HBase, Google Bigtable, ScyllaDB
   - Key traits: column families, high write throughput, distributed by design, IoT/telemetry

8. **vector** (complexityPosition: 0.65)
   - Name: "Vector Database"
   - crossCategory: `[]`
   - CAP: AP
   - Top databases: Pinecone, Weaviate, Milvus, Qdrant, Chroma
   - Key traits: similarity search, embedding storage, ANN algorithms, AI/ML integration
   - Note: Newer category -- ecosystem maturity score should reflect this (lower than relational/document)

9. **newsql** (complexityPosition: 0.72)
   - Name: "NewSQL Database"
   - crossCategory: `["relational"]` (NewSQL = distributed SQL)
   - CAP: CP
   - Top databases: CockroachDB, Google Spanner, TiDB, YugabyteDB
   - Key traits: distributed ACID, SQL interface, horizontal scaling with strong consistency

10. **graph** (complexityPosition: 0.78)
    - Name: "Graph Database"
    - crossCategory: `[]`
    - CAP: CP or Tunable
    - Top databases: Neo4j, Amazon Neptune, ArangoDB, TigerGraph
    - Key traits: relationship-first, traversal queries, pattern matching, knowledge graphs

11. **object** (complexityPosition: 0.82)
    - Name: "Object-Oriented Database"
    - crossCategory: `[]`
    - CAP: CA
    - Top databases: db4o, ObjectDB, Versant, InterSystems Cache
    - Key traits: native object persistence, impedance mismatch elimination, complex object graphs
    - Note: Niche category -- ecosystem maturity and learning curve scores should reflect limited adoption

12. **multi-model** (complexityPosition: 0.88)
    - Name: "Multi-Model Database"
    - crossCategory: `["document", "graph", "key-value", "search"]` (by definition spans multiple models)
    - CAP: Tunable
    - Top databases: ArangoDB, Azure Cosmos DB, FaunaDB, SurrealDB, OrientDB
    - Key traits: multiple data models in one engine, unified query interface, polyglot persistence reduction

**Cross-calibration check after all 12 entries:**
After adding all entries, review each dimension column across all 12 models:
- scalability: key-value and columnar should be highest (9), object should be lowest (2-3)
- performance: key-value and in-memory should be highest (9-10), graph and object should be lower
- reliability: relational should be highest (9), in-memory should be lower (4-5 without persistence)
- operationalSimplicity: key-value should be high (8), graph and multi-model should be lower
- queryFlexibility: relational should be highest (10), key-value should be lowest (2)
- schemaFlexibility: document should be highest (9), relational should be lower (3-4)
- ecosystemMaturity: relational should be highest (10), vector and object should be lowest
- learningCurve: key-value should be easiest (9), graph and multi-model should be hardest (3-4)

Ensure the full 1-10 range is used in each dimension (at least one model near 2-3 and one near 9-10).

**Same format requirements as Task 1:** All fields required, justifications must be specific and technical, top databases must have valid URLs.
  </action>
  <verify>
Run `npx astro build 2>&1 | tail -30` to verify all 12 entries pass Zod validation. Then verify completeness:
1. Count entries: `node -e "const d = require('./src/data/db-compass/models.json'); console.log('Models:', d.length)"` should output 12
2. Check all IDs present: `node -e "const d = require('./src/data/db-compass/models.json'); console.log(d.map(m => m.id).sort().join(', '))"`
3. Check crossCategory populated for multi-model entries: `node -e "const d = require('./src/data/db-compass/models.json'); d.filter(m => m.crossCategory.length > 0).forEach(m => console.log(m.id, '->', m.crossCategory))"`
  </verify>
  <done>
All 12 database model entries exist in models.json. `astro build` succeeds with full Zod validation. Every entry has: 8 dimension scores with justifications, CAP profile with notes, crossCategory links where applicable, 3-6 top databases with descriptions and URLs, strengths/weaknesses/bestFor/avoidWhen lists, summary, and character sketch. Score calibration is consistent across all 12 models within each dimension.
  </done>
</task>

</tasks>

<verification>
1. `npx astro build` succeeds with all 12 dbModels entries validated
2. `models.json` contains exactly 12 entries with IDs: key-value, document, in-memory, time-series, relational, search, columnar, vector, newsql, graph, object, multi-model
3. Every entry has all 8 dimension scores as integers 1-10
4. Every entry has 8 justification strings (none empty)
5. Multi-model databases have crossCategory arrays pointing to valid model IDs
6. Every entry has 3-6 topDatabases with valid URL format
7. Every entry has a CAP classification (CP/AP/CA/Tunable) with notes
8. Score ranges use full 1-10 spectrum within each dimension
</verification>

<success_criteria>
- 12 content-complete database model entries validated by Zod schema at build time
- Scores calibrated one dimension at a time across all models
- Multi-model databases properly cross-referenced
- Top databases with accurate descriptions, licenses, and external URLs
- CAP theorem profiles with nuanced notes (not oversimplified)
- Character sketches add personality and memorability to each model
- Build succeeds end-to-end
</success_criteria>

<output>
After completion, create `.planning/phases/28-data-foundation/28-02-SUMMARY.md`
</output>
